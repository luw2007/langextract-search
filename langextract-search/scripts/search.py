#!/usr/bin/env python3
"""
LangExtract Search - Zhipu + DuckDuckGo + langextract Workflow
é›†æˆæ™ºè°± MCP æœç´¢ã€DuckDuckGo æœç´¢å’Œ langextract ç»“æ„åŒ–æå–
"""

import json
import os
import sys
import argparse
import subprocess
import requests
from datetime import datetime
from pathlib import Path


PROJECT_ROOT = Path(__file__).parent.parent
SCRIPTS_DIR = Path(__file__).parent


def add_project_path():
    """å°† scripts ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ langextractã€‚"""
    if str(SCRIPTS_DIR) not in sys.path:
        sys.path.append(str(SCRIPTS_DIR))



def resolve_api_key(value):
    if value is None:
        return None
    if not isinstance(value, str):
        return value
    key = value.strip()
    if not key:
        return None
    env_value = os.getenv(key)
    if env_value:
        return env_value
    if key.startswith("${") and key.endswith("}") and len(key) > 3:
        env_value = os.getenv(key[2:-1])
        if env_value:
            return env_value
    if key.startswith("$") and len(key) > 1:
        env_value = os.getenv(key[1:])
        if env_value:
            return env_value
    return key


def map_timelimit(value: str, target: str) -> str:
    """
    å°†ç»Ÿä¸€çš„æ—¶é—´è¿‡æ»¤å€¼æ˜ å°„åˆ°ç›®æ ‡æœç´¢å¼•æ“çš„æ ¼å¼ã€‚
    
    Args:
        value: ç»Ÿä¸€å€¼ (day/week/month/year/null/none/None)
        target: ç›®æ ‡å¼•æ“ (ddgs/zai)
    
    Returns:
        æ˜ å°„åçš„å€¼
    """
    if value is None or str(value).lower() in ('null', 'none', ''):
        return None if target == 'ddgs' else 'noLimit'
    
    mapping = {
        'ddgs': {'day': 'd', 'week': 'w', 'month': 'm', 'year': 'y'},
        'zai': {'day': 'oneDay', 'week': 'oneWeek', 'month': 'oneMonth', 'year': 'oneYear'}
    }
    
    return mapping.get(target, {}).get(str(value).lower(), value)


def get_langextract_config(conf: dict = None) -> dict:
    """
    è·å– langextract é…ç½®ã€‚
    
    Returns:
        dict: {provider, model, baseUrl, apiKey}
    
    Raises:
        ValueError: é…ç½®ç¼ºå¤±æ—¶æŠ›å‡º
    """
    if conf is None:
        conf = load_project_conf()
    
    task_conf = conf.get('langextract', {})
    
    if not task_conf:
        raise ValueError("langextract é…ç½®ç¼ºå¤±ã€‚è¯·åœ¨ conf.json ä¸­é…ç½® langextract èŠ‚ç‚¹ï¼Œå‚è€ƒ conf.json.example")
    
    provider = task_conf.get('provider')
    model = task_conf.get('model')
    base_url = task_conf.get('baseUrl')
    api_key = resolve_api_key(task_conf.get('apiKey'))
    
    missing = []
    if not model:
        missing.append('model')
    if not base_url:
        missing.append('baseUrl')
    if not api_key:
        missing.append('apiKey')
    
    if missing:
        raise ValueError(f"langextract é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing)}ã€‚è¯·å‚è€ƒ conf.json.example")
    
    return {
        'provider': provider,
        'model': model,
        'baseUrl': base_url,
        'apiKey': api_key
    }


def get_zhipu_search_config(conf: dict = None) -> dict:
    """
    è·å–æ™ºè°±æœç´¢é…ç½®ã€‚
    
    é…ç½®é¡¹:
        enabled: æ˜¯å¦å¯ç”¨æ™ºè°±æœç´¢ï¼Œé»˜è®¤ True
        apiKey: API Keyï¼ˆæ”¯æŒç¯å¢ƒå˜é‡å¼•ç”¨ï¼‰
        search_engine: æœç´¢å¼•æ“ search_std/search_pro/search_pro_sogou/search_pro_quarkï¼Œé»˜è®¤ search_pro
        count: è¿”å›ç»“æœæ•° 1-50ï¼Œé»˜è®¤ 15
        timelimit: æ—¶é—´è¿‡æ»¤ day/week/month/year/nullï¼Œé»˜è®¤ nullï¼ˆä¸é™ï¼‰
        content_size: å†…å®¹é•¿åº¦ medium/highï¼Œé»˜è®¤ high
        search_domain_filter: é™å®šæœç´¢åŸŸåï¼Œé»˜è®¤ null
    """
    if conf is None:
        conf = load_project_conf()
    
    search_conf = conf.get('zhipu_search', {})
    
    api_key = resolve_api_key(search_conf.get('apiKey'))
    timelimit = search_conf.get('timelimit')
    
    return {
        'enabled': search_conf.get('enabled', True),
        'apiKey': api_key,
        'search_engine': search_conf.get('search_engine', 'search_pro'),
        'count': search_conf.get('count', 15),
        'timelimit': timelimit,
        'timelimit_mapped': map_timelimit(timelimit, 'zai'),
        'content_size': search_conf.get('content_size', 'high'),
        'search_domain_filter': search_conf.get('search_domain_filter')
    }


def get_duckduckgo_search_config(conf: dict = None) -> dict:
    """
    è·å– DuckDuckGo æœç´¢é…ç½®ã€‚
    
    é…ç½®é¡¹:
        enabled: æ˜¯å¦å¯ç”¨ DuckDuckGo æœç´¢ï¼Œé»˜è®¤ True
        maxResults: è¿”å›ç»“æœæ•°ï¼Œé»˜è®¤ 20
        region: åœ°åŒºä»£ç  cn-zh/us-en/wt-wt ç­‰ï¼Œé»˜è®¤ wt-wtï¼ˆæ— é™åˆ¶ï¼‰
        safesearch: å®‰å…¨æœç´¢ on/moderate/offï¼Œé»˜è®¤ moderate
        timelimit: æ—¶é—´è¿‡æ»¤ day/week/month/year/nullï¼Œé»˜è®¤ nullï¼ˆä¸é™ï¼‰
        backend: æœç´¢åç«¯ auto/bing/google/duckduckgo/brave/yandex/yahooï¼Œé»˜è®¤ auto
        proxy: ä»£ç†åœ°å€ï¼Œé»˜è®¤ null
        timeout: è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 10
    """
    if conf is None:
        conf = load_project_conf()
    
    search_conf = conf.get('duckduckgo_search', {})
    timelimit = search_conf.get('timelimit')
    
    return {
        'enabled': search_conf.get('enabled', True),
        'maxResults': search_conf.get('maxResults', 20),
        'region': search_conf.get('region', 'wt-wt'),
        'safesearch': search_conf.get('safesearch', 'moderate'),
        'timelimit': timelimit,
        'timelimit_mapped': map_timelimit(timelimit, 'ddgs'),
        'backend': search_conf.get('backend', 'auto'),
        'proxy': search_conf.get('proxy'),
        'timeout': search_conf.get('timeout', 10)
    }


def get_volcengine_search_config(conf: dict = None) -> dict:
    """è·å–ç«å±±å¼•æ“è”ç½‘é—®ç­”é…ç½®ã€‚"""
    if conf is None:
        conf = load_project_conf()
    
    search_conf = conf.get('volcengine_search', {})
    
    api_key = resolve_api_key(search_conf.get('apiKey'))
    bot_id = resolve_api_key(search_conf.get('botId'))
    
    return {
        'enabled': search_conf.get('enabled', False),
        'apiKey': api_key,
        'botId': bot_id
    }


def get_extraction_config(conf: dict = None) -> dict:
    """è·å–ç»“æ„åŒ–æå–é…ç½®ã€‚"""
    if conf is None:
        conf = load_project_conf()
    
    extraction_conf = conf.get('extraction', {})
    
    return {
        'max_content_length': extraction_conf.get('max_content_length', 70000)
    }


def get_project_conf_path():
    """è·å–é¡¹ç›®é…ç½®æ–‡ä»¶è·¯å¾„"""
    return PROJECT_ROOT / "conf.json"


def load_project_conf():
    """åŠ è½½é¡¹ç›®é…ç½®"""
    conf_path = get_project_conf_path()
    if conf_path.exists():
        try:
            with open(conf_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ conf.json æ ¼å¼é”™è¯¯: {e}")
            print(f"   è¯·ä¿®å¤æˆ–åˆ é™¤ {conf_path} åé‡è¯•")
            raise
    return {}


def parse_mcp_output(output: str):
    """Parse MCP output - handle double-quoted JSON."""
    try:
        # First parse: get the string
        first_parse = json.loads(output.strip())
        if isinstance(first_parse, str):
            # Second parse: parse the string to get the array
            return json.loads(first_parse)
        elif isinstance(first_parse, list):
            # Already an array
            return first_parse
        else:
            raise ValueError(f"Unexpected type: {type(first_parse)}")
    except Exception as e:
        print(f"   è§£æè°ƒè¯•: {e}")
        print(f"   åŸå§‹è¾“å‡º: {repr(output[:200])}")
        raise


def search_with_zhipu_mcp(query: str, verbose: bool = False):
    """
    Step 1a: Search using Zhipu AI's official zai-sdk (web_search API).
    
    é…ç½®å‚æ•°ä» conf.json çš„ zhipu_search èŠ‚ç‚¹è¯»å–:
        search_engine: æœç´¢å¼•æ“
        count: è¿”å›ç»“æœæ•°
        timelimit: æ—¶é—´è¿‡æ»¤
        content_size: å†…å®¹é•¿åº¦
        search_domain_filter: åŸŸåè¿‡æ»¤
    """
    search_conf = get_zhipu_search_config()
    
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ” æ­¥éª¤ 1a: æ™ºè°± AI ç½‘ç»œæœç´¢")
        print("=" * 60)
        print(f"\nğŸ“¥ è¾“å…¥:")
        print(f"   æœç´¢æŸ¥è¯¢: {query}")
        print(f"   æœç´¢å¼•æ“: {search_conf['search_engine']}")
        print(f"   ç»“æœæ•°é‡: {search_conf['count']}")
        print(f"   æ—¶é—´è¿‡æ»¤: {search_conf['timelimit']} -> {search_conf['timelimit_mapped']}")
        print(f"   å†…å®¹é•¿åº¦: {search_conf['content_size']}")
        if search_conf['search_domain_filter']:
            print(f"   åŸŸåè¿‡æ»¤: {search_conf['search_domain_filter']}")
    
    try:
        try:
            from zai import ZhipuAiClient
            has_zai = True
        except ImportError:
            has_zai = False
        
        api_key = search_conf.get('apiKey')
        
        if not api_key:
            raise ValueError("æ™ºè°±æœç´¢ API Key æœªé…ç½®ã€‚è¯·åœ¨ conf.json çš„ zhipu_search.apiKey ä¸­è®¾ç½®")
        
        if verbose:
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨æ™ºè°±æœç´¢ API...")
            print(f"   ä½¿ç”¨ zai-sdk: {has_zai}")
        
        search_results = []
        
        if has_zai:
            client = ZhipuAiClient(api_key=api_key)
            
            search_params = {
                'search_engine': search_conf['search_engine'],
                'search_query': query,
                'count': search_conf['count'],
                'search_recency_filter': search_conf['timelimit_mapped'],
                'content_size': search_conf['content_size']
            }
            if search_conf['search_domain_filter']:
                search_params['search_domain_filter'] = search_conf['search_domain_filter']
            
            response = client.web_search.web_search(**search_params)
            
            # Parse search results from the response
            if hasattr(response, 'search_result') and response.search_result:
                for item in response.search_result:
                    search_results.append({
                        "title": getattr(item, "title", ""),
                        "link": getattr(item, "link", ""),
                        "content": getattr(item, "content", ""),
                        "publish_date": getattr(item, "publish_date", ""),
                        "site_name": getattr(item, "media", "")
                    })
            # Also check if response is a dict-like object
            elif isinstance(response, dict) and 'search_result' in response:
                for item in response['search_result']:
                    search_results.append({
                        "title": item.get("title", ""),
                        "link": item.get("link", ""),
                        "content": item.get("content", ""),
                        "publish_date": item.get("publish_date", ""),
                        "site_name": item.get("media", "")
                    })
        
        # If no results from SDK, fall back to DuckDuckGo
        if not search_results:
            if verbose:
                print(f"   æ™ºè°±æœç´¢æœªè·å–åˆ°ç»“æœï¼Œå°†ä½¿ç”¨ DuckDuckGo...")
            
            # Return empty result so the workflow can use DuckDuckGo instead
            return {
                "success": False,
                "error": "Zhipu search did not return results, will use DuckDuckGo",
                "query": query,
                "source": "zhipu"
            }
        
        if verbose:
            print(f"\nğŸ“¤ è¾“å‡º:")
            print(f"   æ™ºè°±æœç´¢æˆåŠŸ: âœ…")
            print(f"   æ‰¾åˆ°ç»“æœ: {len(search_results)} æ¡")
            
            for i, item in enumerate(search_results[:3], 1):
                print(f"\n   {i}. {item.get('title', 'No title')}")
                print(f"      URL: {item.get('link', 'No link')}")
                print(f"      æ—¥æœŸ: {item.get('publish_date', 'No date')}")
                content = item.get('content', '')
                print(f"      æ‘˜è¦: {content[:100]}...")
        
        # Combine search results into a single text
        combined_content = ""
        for item in search_results:
            title = item.get('title', '')
            link = item.get('link', '')
            content = item.get('content', '')
            date = item.get('publish_date', '')
            
            combined_content += f"# [æ™ºè°±] {title}\n"
            if date:
                combined_content += f"æ—¥æœŸ: {date}\n"
            if link:
                combined_content += f"é“¾æ¥: {link}\n"
            combined_content += f"\n{content}\n\n"
        
        return {
            "success": True,
            "query": query,
            "search_results": search_results,
            "combined_content": combined_content,
            "source": "zhipu"
        }
        
    except Exception as e:
        if verbose:
            print(f"\nâŒ æ™ºè°±æœç´¢å¤±è´¥: {e}")
            print(f"   å°†ä½¿ç”¨ DuckDuckGo ä½œä¸ºæ›¿ä»£...")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "query": query,
            "source": "zhipu"
        }


def search_with_volcengine(query: str, verbose: bool = False):
    """
    Step 1c: Search using Volcengine (ç«å±±å¼•æ“è”ç½‘é—®ç­”Agent API).
    
    æ–‡æ¡£: https://www.volcengine.com/docs/85508/1510834
    æ¥å…¥æ–¹å¼: APIKeyæ¥å…¥
    URL: https://open.feedcoopapi.com/agent_api/agent/chat/completion
    """
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ” æ­¥éª¤ 1c: ç«å±±å¼•æ“è”ç½‘é—®ç­”æœç´¢")
        print("=" * 60)
        print(f"\nğŸ“¥ è¾“å…¥:")
        print(f"   æœç´¢æŸ¥è¯¢: {query}")
    
    try:
        search_conf = get_volcengine_search_config()
        api_key = search_conf.get('apiKey')
        bot_id = search_conf.get('botId')
        
        if not api_key:
            raise ValueError("ç«å±±å¼•æ“æœç´¢ API Key æœªé…ç½®ã€‚è¯·åœ¨ conf.json çš„ volcengine_search.apiKey ä¸­è®¾ç½®")
        if not bot_id:
            raise ValueError("ç«å±±å¼•æ“ Bot ID æœªé…ç½®ã€‚è¯·åœ¨ conf.json çš„ volcengine_search.botId ä¸­è®¾ç½®")
        
        if verbose:
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨ç«å±±å¼•æ“è”ç½‘é—®ç­” API...")
            print(f"   Bot ID: {bot_id}")
        
        url = "https://open.feedcoopapi.com/agent_api/agent/chat/completion"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "bot_id": bot_id,
            "messages": [
                {"role": "user", "content": query}
            ],
            "stream": False
        }
        
        response = requests.post(
            url,
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        result = response.json()
        
        search_results = []
        combined_content = ""
        answer_content = ""
        
        if result.get("code") == 0 and result.get("data"):
            data = result["data"]
            
            if "answer" in data:
                answer_content = data["answer"]
                combined_content += f"# [ç«å±±å¼•æ“] è”ç½‘é—®ç­”ç»“æœ\n\n{answer_content}\n\n"
            
            if "references" in data and data["references"]:
                for ref in data["references"]:
                    search_results.append({
                        "title": ref.get("title", ""),
                        "link": ref.get("url", ""),
                        "content": ref.get("content", ref.get("summary", "")),
                        "site_name": ref.get("site_name", "")
                    })
                    combined_content += f"## [{ref.get('site_name', 'å‚è€ƒ')}] {ref.get('title', '')}\n"
                    combined_content += f"é“¾æ¥: {ref.get('url', '')}\n"
                    combined_content += f"{ref.get('content', ref.get('summary', ''))}\n\n"
            
            if "search_results" in data and data["search_results"]:
                for item in data["search_results"]:
                    search_results.append({
                        "title": item.get("title", ""),
                        "link": item.get("url", item.get("link", "")),
                        "content": item.get("content", item.get("snippet", "")),
                        "site_name": item.get("site_name", "")
                    })
        
        if verbose:
            print(f"\nğŸ“¤ è¾“å‡º:")
            print(f"   ç«å±±å¼•æ“æœç´¢æˆåŠŸ: âœ…")
            print(f"   æ‰¾åˆ°å‚è€ƒç»“æœ: {len(search_results)} æ¡")
            if answer_content:
                print(f"   å›ç­”å†…å®¹é•¿åº¦: {len(answer_content)} å­—ç¬¦")
                print(f"   å›ç­”é¢„è§ˆ: {answer_content[:200]}...")
            
            for i, item in enumerate(search_results[:3], 1):
                print(f"\n   {i}. {item.get('title', 'No title')}")
                print(f"      URL: {item.get('link', 'No link')}")
                content = item.get('content', '')
                if content:
                    print(f"      æ‘˜è¦: {content[:100]}...")
        
        return {
            "success": True,
            "query": query,
            "search_results": search_results,
            "combined_content": combined_content,
            "answer": answer_content,
            "source": "volcengine"
        }
        
    except Exception as e:
        if verbose:
            print(f"\nâŒ ç«å±±å¼•æ“æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "query": query,
            "source": "volcengine"
        }


def search_with_duckduckgo(query: str, verbose: bool = False, max_results: int = None):
    """
    Step 1b: Search using DuckDuckGo (ddgs).
    
    é…ç½®å‚æ•°ä» conf.json çš„ duckduckgo_search èŠ‚ç‚¹è¯»å–:
        maxResults: è¿”å›ç»“æœæ•°
        region: åœ°åŒºä»£ç 
        safesearch: å®‰å…¨æœç´¢
        timelimit: æ—¶é—´è¿‡æ»¤
        backend: æœç´¢åç«¯
        proxy: ä»£ç†åœ°å€
        timeout: è¯·æ±‚è¶…æ—¶
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        verbose: æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        max_results: è¦†ç›–é…ç½®çš„ç»“æœæ•°ï¼ˆå¯é€‰ï¼‰
    """
    search_conf = get_duckduckgo_search_config()
    actual_max_results = max_results if max_results is not None else search_conf['maxResults']
    
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ” æ­¥éª¤ 1b: DuckDuckGo æœç´¢")
        print("=" * 60)
        print(f"\nğŸ“¥ è¾“å…¥:")
        print(f"   æœç´¢æŸ¥è¯¢: {query}")
        print(f"   ç»“æœæ•°é‡: {actual_max_results}")
        print(f"   åœ°åŒºä»£ç : {search_conf['region']}")
        print(f"   å®‰å…¨æœç´¢: {search_conf['safesearch']}")
        print(f"   æ—¶é—´è¿‡æ»¤: {search_conf['timelimit']} -> {search_conf['timelimit_mapped']}")
        print(f"   æœç´¢åç«¯: {search_conf['backend']}")
        if search_conf['proxy']:
            print(f"   ä»£ç†åœ°å€: {search_conf['proxy']}")
        print(f"   è¶…æ—¶è®¾ç½®: {search_conf['timeout']}s")
    
    try:
        from ddgs import DDGS
        
        if verbose:
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨ DuckDuckGo...")
        
        ddgs_kwargs = {'timeout': search_conf['timeout']}
        if search_conf['proxy']:
            ddgs_kwargs['proxy'] = search_conf['proxy']
        
        with DDGS(**ddgs_kwargs) as ddgs:
            search_params = {
                'query': query,
                'max_results': actual_max_results,
                'region': search_conf['region'],
                'safesearch': search_conf['safesearch'],
                'backend': search_conf['backend']
            }
            if search_conf['timelimit_mapped']:
                search_params['timelimit'] = search_conf['timelimit_mapped']
            
            search_results = list(ddgs.text(**search_params))
        
        if verbose:
            print(f"\nğŸ“¤ è¾“å‡º:")
            print(f"   DuckDuckGo æœç´¢æˆåŠŸ: âœ…")
            print(f"   æ‰¾åˆ°ç»“æœ: {len(search_results)} æ¡")
            
            for i, item in enumerate(search_results[:3], 1):
                print(f"\n   {i}. {item.get('title', 'No title')}")
                print(f"      URL: {item.get('href', 'No link')}")
                content = item.get('body', '')
                print(f"      æ‘˜è¦: {content[:100]}...")
        
        # Combine search results into a single text
        combined_content = ""
        for item in search_results:
            title = item.get('title', '')
            link = item.get('href', '')
            content = item.get('body', '')
            
            combined_content += f"# [DuckDuckGo] {title}\n"
            if link:
                combined_content += f"é“¾æ¥: {link}\n"
            combined_content += f"\n{content}\n\n"
        
        return {
            "success": True,
            "query": query,
            "search_results": search_results,
            "combined_content": combined_content,
            "source": "duckduckgo"
        }
        
    except Exception as e:
        if verbose:
            print(f"\nâŒ DuckDuckGo æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "query": query,
            "source": "duckduckgo"
        }


def extract_with_langextract(zhipu_data, ddg_data, volcengine_data=None, verbose: bool = False):
    """
    Step 2: Extract structured information using configured model (doubao/glm/zhipu).
    """
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ“ æ­¥éª¤ 2: ç»“æ„åŒ–æå–")
        print("=" * 60)
    
    volcengine_data = volcengine_data or {}
    
    combined_content = ""
    if zhipu_data.get("success"):
        combined_content += zhipu_data["combined_content"]
    if ddg_data.get("success"):
        combined_content += ddg_data["combined_content"]
    if volcengine_data.get("success"):
        combined_content += volcengine_data["combined_content"]
    
    extraction_config = get_extraction_config()
    max_content_length = extraction_config['max_content_length']
    if len(combined_content) > max_content_length:
        if verbose:
            print(f"âš ï¸ å†…å®¹è¿‡é•¿ ({len(combined_content)} å­—ç¬¦)ï¼Œæˆªæ–­è‡³ {max_content_length} å­—ç¬¦")
        combined_content = combined_content[:max_content_length]
    
    if not combined_content:
        if verbose:
            print("âŒ æ‰€æœ‰æœç´¢éƒ½å¤±è´¥ï¼Œæ— æ³•æå–ä¿¡æ¯")
        return {
            "success": False,
            "error": "All searches failed",
            "zhipu_data": zhipu_data,
            "ddg_data": ddg_data,
            "volcengine_data": volcengine_data
        }
    
    try:
        model_config = get_langextract_config()
        model_provider = model_config['provider']
        model_name = model_config['model']
        base_url = model_config['baseUrl']
        api_key = model_config['apiKey']
        
        if not api_key:
            raise ValueError("langextract API Key æœªé…ç½®ã€‚è¯·åœ¨ conf.json çš„ langextract.apiKey ä¸­è®¾ç½®")
        
        if verbose:
            print(f"\nğŸ“¥ è¾“å…¥:")
            print(f"   æ€»æœç´¢å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")
            print(f"   æ¨¡å‹æä¾›å•†: {model_provider}")
            print(f"   æ¨¡å‹åç§°: {model_name}")
            print(f"   Base URL: {base_url}")
        
        extraction_prompt = f"""åŸºäºä»¥ä¸‹ç½‘ç»œæœç´¢ç»“æœï¼ˆåŒ…å«æ™ºè°±ã€DuckDuckGoã€ç«å±±å¼•æ“çš„ç»“æœï¼‰ï¼Œè¯·æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š

æœç´¢ç»“æœï¼š
{combined_content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦å†…å®¹æ‘˜è¦
2. å…³é”®ç‚¹åˆ—è¡¨ï¼ˆ3-5ä¸ªï¼‰
3. ç›¸å…³äº‹å®æˆ–æ•°æ®
4. æ¥æºæˆ–å‚è€ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰

è¯·ç”¨æ¸…æ™°çš„æ ¼å¼è¾“å‡ºã€‚"""
        
        if verbose:
            print(f"\nğŸ¤– æ­£åœ¨è°ƒç”¨ {model_provider} API...")
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model_name,
            "messages": [
                {
                    "role": "user",
                    "content": extraction_prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 2000,
            "top_p": 0.9
        }
        
        response = requests.post(
            f"{base_url}/chat/completions",
            headers=headers,
            json=payload,
            timeout=120
        )
        response.raise_for_status()
        
        result = response.json()
        extracted_info = result["choices"][0]["message"]["content"]
        
        if verbose:
            print(f"\nğŸ“¤ è¾“å‡º:")
            print(f"   æå–æˆåŠŸ: âœ…")
            print(f"   æå–å†…å®¹é•¿åº¦: {len(extracted_info)} å­—ç¬¦")
            print(f"\n   æå–å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰:")
            print(f"   {extracted_info[:500]}...")
        
        return {
            "success": True,
            "zhipu_data": zhipu_data,
            "ddg_data": ddg_data,
            "volcengine_data": volcengine_data,
            "combined_content": combined_content,
            "extracted_info": extracted_info,
            "model_provider": model_provider,
            "model_name": model_name,
            "input": {
                "total_content_length": len(combined_content),
                "extraction_prompt": extraction_prompt[:200] + "..."
            }
        }
        
    except Exception as e:
        if verbose:
            print(f"\nâŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "zhipu_data": zhipu_data,
            "ddg_data": ddg_data,
            "volcengine_data": volcengine_data
        }


def save_results(final_result, output_dir: str, save_json: bool = False, verbose: bool = False):
    """Save results to files."""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if verbose:
        print("\n" + "=" * 60)
        print("ğŸ’¾ ä¿å­˜ç»“æœ")
        print("=" * 60)
        print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    
    saved_files = []
    
    # Save Zhipu search result
    if final_result.get("zhipu_data", {}).get("success"):
        zhipu_file = output_path / f"zhipu_search_result_{timestamp}.md"
        with open(zhipu_file, "w", encoding="utf-8") as f:
            f.write(f"# æ™ºè°± MCP æœç´¢ç»“æœ\n\n")
            f.write(f"**æŸ¥è¯¢**: {final_result['zhipu_data']['query']}\n\n")
            f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(final_result['zhipu_data']['combined_content'])
        saved_files.append(str(zhipu_file))
        if verbose:
            print(f"âœ… å·²ä¿å­˜: {zhipu_file.name}")
    
    # Save DuckDuckGo search result
    if final_result.get("ddg_data", {}).get("success"):
        ddg_file = output_path / f"duckduckgo_search_result_{timestamp}.md"
        with open(ddg_file, "w", encoding="utf-8") as f:
            f.write(f"# DuckDuckGo æœç´¢ç»“æœ\n\n")
            f.write(f"**æŸ¥è¯¢**: {final_result['ddg_data']['query']}\n\n")
            f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(final_result['ddg_data']['combined_content'])
        saved_files.append(str(ddg_file))
        if verbose:
            print(f"âœ… å·²ä¿å­˜: {ddg_file.name}")
    
    # Save Volcengine search result
    if final_result.get("volcengine_data", {}).get("success"):
        volcengine_file = output_path / f"volcengine_search_result_{timestamp}.md"
        with open(volcengine_file, "w", encoding="utf-8") as f:
            f.write(f"# ç«å±±å¼•æ“è”ç½‘é—®ç­”æœç´¢ç»“æœ\n\n")
            f.write(f"**æŸ¥è¯¢**: {final_result['volcengine_data']['query']}\n\n")
            f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(final_result['volcengine_data']['combined_content'])
        saved_files.append(str(volcengine_file))
        if verbose:
            print(f"âœ… å·²ä¿å­˜: {volcengine_file.name}")
    
    # Save extracted info
    if final_result.get("success") and final_result.get("extracted_info"):
        extract_file = output_path / f"extracted_info_{timestamp}.md"
        with open(extract_file, "w", encoding="utf-8") as f:
            f.write(f"# æå–çš„ç»“æ„åŒ–ä¿¡æ¯\n\n")
            f.write(f"**æºæŸ¥è¯¢**: {final_result['zhipu_data']['query'] if final_result.get('zhipu_data') else final_result['ddg_data']['query']}\n\n")
            f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")
            f.write(final_result['extracted_info'])
        saved_files.append(str(extract_file))
        if verbose:
            print(f"âœ… å·²ä¿å­˜: {extract_file.name}")
    
    # Save workflow summary
    summary_file = output_path / f"workflow_summary_{timestamp}.md"
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(f"# å·¥ä½œæµæ‘˜è¦\n\n")
        f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
        f.write(f"**çŠ¶æ€**: {'âœ… æˆåŠŸ' if final_result.get('success') else 'âŒ å¤±è´¥'}\n\n")
        
        if final_result.get("success"):
            query = (
                final_result.get('zhipu_data', {}).get('query') or 
                final_result.get('ddg_data', {}).get('query') or
                final_result.get('volcengine_data', {}).get('query') or
                ''
            )
            f.write(f"**æŸ¥è¯¢**: {query}\n\n")
            if final_result.get("zhipu_data", {}).get("success"):
                f.write(f"**æ™ºè°±æœç´¢ç»“æœæ•°**: {len(final_result['zhipu_data'].get('search_results', []))} æ¡\n\n")
            if final_result.get("ddg_data", {}).get("success"):
                f.write(f"**DuckDuckGo æœç´¢ç»“æœæ•°**: {len(final_result['ddg_data'].get('search_results', []))} æ¡\n\n")
            if final_result.get("volcengine_data", {}).get("success"):
                f.write(f"**ç«å±±å¼•æ“æœç´¢ç»“æœæ•°**: {len(final_result['volcengine_data'].get('search_results', []))} æ¡\n\n")
            f.write(f"**æ€»æœç´¢å†…å®¹é•¿åº¦**: {len(final_result['combined_content'])} å­—ç¬¦\n\n")
            if final_result.get("extracted_info"):
                f.write(f"**æå–å†…å®¹é•¿åº¦**: {len(final_result['extracted_info'])} å­—ç¬¦\n\n")
        
        if final_result.get("error"):
            f.write(f"**é”™è¯¯**: {final_result['error']}\n\n")
        
        if final_result.get("warning"):
            f.write(f"**è­¦å‘Š**: {final_result['warning']}\n\n")
    
    saved_files.append(str(summary_file))
    if verbose:
        print(f"âœ… å·²ä¿å­˜: {summary_file.name}")
    
    # Save full JSON
    if save_json:
        json_file = output_path / f"full_results_{timestamp}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        saved_files.append(str(json_file))
        if verbose:
            print(f"âœ… å·²ä¿å­˜: {json_file.name}")
    
    return saved_files


def main():
    add_project_path()
    
    parser = argparse.ArgumentParser(
        description="æ™ºè°± MCP + DuckDuckGo + ç«å±±å¼•æ“ + è±†åŒ… æœç´¢æå–å·¥ä½œæµ"
    )
    parser.add_argument(
        "query",
        nargs="?",
        help="æœç´¢å…³é”®è¯ï¼ˆä¹Ÿå¯ä»¥ç”¨ --query æŒ‡å®šï¼‰"
    )
    parser.add_argument(
        "--query",
        help="æœç´¢å…³é”®è¯"
    )
    parser.add_argument(
        "--save-json",
        action="store_true",
        help="ä¿å­˜å®Œæ•´çš„ JSON ç»“æœ"
    )
    parser.add_argument(
        "--output-dir",
        default=str(PROJECT_ROOT / "output"),
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†çš„è¾“å…¥å’Œè¾“å‡ºä¿¡æ¯ï¼ˆéªŒè¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--ddg-max-results",
        type=int,
        default=None,
        help="DuckDuckGo æœ€å¤§æœç´¢ç»“æœæ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨ conf.json ä¸­çš„ maxResultsï¼‰"
    )
    parser.add_argument(
        "--volcengine",
        action="store_true",
        help="å¯ç”¨ç«å±±å¼•æ“è”ç½‘é—®ç­”æœç´¢ï¼ˆéœ€åœ¨ conf.json ä¸­é…ç½® volcengine_searchï¼‰"
    )
    parser.add_argument(
        "--volcengine-only",
        action="store_true",
        help="ä»…ä½¿ç”¨ç«å±±å¼•æ“æœç´¢ï¼ˆä¸ä½¿ç”¨æ™ºè°±å’ŒDuckDuckGoï¼‰"
    )
    
    args = parser.parse_args()
    
    # è·å–æŸ¥è¯¢å…³é”®è¯ï¼šä½ç½®å‚æ•°æˆ– --query å‚æ•°äºŒé€‰ä¸€
    search_query = args.query
    
    if not search_query:
        print("âŒ è¯·æä¾›æœç´¢å…³é”®è¯ï¼")
        parser.print_help()
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸ”„ æ™ºè°± MCP + DuckDuckGo + ç«å±±å¼•æ“ + è±†åŒ… å·¥ä½œæµ")
    print("=" * 60)
    
    zhipu_result = {}
    ddg_result = {}
    volcengine_result = {}
    
    if args.volcengine_only:
        volcengine_result = search_with_volcengine(search_query, verbose=args.verbose)
    else:
        zhipu_search_conf = get_zhipu_search_config()
        if zhipu_search_conf.get('enabled', True):
            zhipu_result = search_with_zhipu_mcp(search_query, verbose=args.verbose)
        else:
            if args.verbose:
                print("\nâ­ï¸ æ™ºè°±æœç´¢å·²ç¦ç”¨ï¼Œè·³è¿‡...")
        ddg_result = search_with_duckduckgo(search_query, verbose=args.verbose, max_results=args.ddg_max_results)
        
        if args.volcengine:
            volcengine_result = search_with_volcengine(search_query, verbose=args.verbose)
    
    final_result = extract_with_langextract(
        zhipu_result, ddg_result, volcengine_result, verbose=args.verbose
    )
    
    # Save results
    saved_files = save_results(
        final_result,
        args.output_dir,
        save_json=args.save_json,
        verbose=args.verbose
    )
    
    # Final summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ å·¥ä½œæµå®Œæˆ")
    print("=" * 60)
    
    if final_result.get("success"):
        print(f"\nâœ… å·¥ä½œæµæˆåŠŸï¼")
        print(f"   æŸ¥è¯¢: {search_query}")
        if final_result.get("zhipu_data", {}).get("success"):
            print(f"   æ™ºè°±æœç´¢ç»“æœ: {len(zhipu_result.get('search_results', []))} æ¡")
        if final_result.get("ddg_data", {}).get("success"):
            print(f"   DuckDuckGo æœç´¢ç»“æœ: {len(ddg_result.get('search_results', []))} æ¡")
        if final_result.get("volcengine_data", {}).get("success"):
            print(f"   ç«å±±å¼•æ“æœç´¢ç»“æœ: {len(volcengine_result.get('search_results', []))} æ¡")
        print(f"   ä¿å­˜æ–‡ä»¶: {len(saved_files)} ä¸ª")
        for f in saved_files:
            print(f"   - {Path(f).name}")
        
        print("\n" + "=" * 60)
        print("ğŸ“ æå–çš„ä¿¡æ¯")
        print("=" * 60)
        print("\n" + final_result["extracted_info"])
    else:
        print(f"\nâŒ å·¥ä½œæµå¤±è´¥: {final_result.get('error', 'Unknown error')}")
    
    print("\n" + "=" * 60)


if __name__ == '__main__':
    main()
